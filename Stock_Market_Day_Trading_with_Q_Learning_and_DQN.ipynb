{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bHTlzdh4x6UT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Group 6 \\\\\n",
        "Νικολόπουλος Ευάγγελος \\\\\n",
        "Γεώργιος Σάντης"
      ],
      "metadata": {
        "id": "kJjbq__Q0DX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "##Scenario of Assignment 3 Question 1\n",
        "\n",
        "###Code from Assignment 3 (Policy Iteration - For Comparison)"
      ],
      "metadata": {
        "id": "21miCn7qw9kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "gamma = 0\n",
        "r1H = 0.1\n",
        "r1L = -0.02\n",
        "r2H = 0.05\n",
        "r2L = -0.01\n",
        "c = 0.12\n",
        "p1HH = 0.9\n",
        "p1HL = 0.1\n",
        "p1LL = 0.9\n",
        "p1LH = 0.1\n",
        "p2HH = 0.9\n",
        "p2HL = 0.1\n",
        "p2LL = 0.9\n",
        "p2LH = 0.1\n",
        "N = 2\n",
        "\n",
        "rewards = {\n",
        "    (1, 'H'): r1H,\n",
        "    (1, 'L'): r1L,\n",
        "    (2, 'H'): r2H,\n",
        "    (2, 'L'): r2L\n",
        "}\n",
        "\n",
        "transitions = {\n",
        "    (1, 'H'): {'H': p1HH, 'L': p1HL},\n",
        "    (1, 'L'): {'H': p1LH, 'L': p1LL},\n",
        "    (2, 'H'): {'H': p2HH, 'L': p2HL},\n",
        "    (2, 'L'): {'H': p2LH, 'L': p2LL}\n",
        "}\n",
        "\n",
        "states = [(1, 'H', 'H'), (1, 'H', 'L'), (1, 'L', 'H'), (1, 'L', 'L'),\n",
        "          (2, 'H', 'H'), (2, 'H', 'L'), (2, 'L', 'H'), (2, 'L', 'L')]\n",
        "\n",
        "actions = ['keep', 'switch']\n",
        "\n",
        "def get_state_index(state):\n",
        "    return states.index(state)\n",
        "\n",
        "def get_transition_prob_reward(state, action):\n",
        "    i, stock_state1, stock_state2 = state\n",
        "    next_states = []\n",
        "    stock_states = [stock_state1, stock_state2]\n",
        "\n",
        "    if action == 'keep':\n",
        "        if stock_states[i - 1] == 'H':\n",
        "            next_states.append(((i, stock_states[i - 1], stock_states[1 - (i - 1)]), transitions[(i, 'H')]['H'], rewards[(i, 'H')]))\n",
        "            stock_states[i - 1] = 'L'\n",
        "            next_states.append(((i, stock_states[i - 1], stock_states[1 - (i - 1)]), transitions[(i, 'H')]['L'], rewards[(i, 'H')]))\n",
        "        else:\n",
        "            next_states.append(((i, stock_states[i - 1], stock_states[1 - (i - 1)]), transitions[(i, 'L')]['H'], rewards[(i, 'L')]))\n",
        "            stock_states[i - 1] = 'H'\n",
        "            next_states.append(((i, stock_states[i - 1], stock_states[1 - (i - 1)]), transitions[(i, 'L')]['L'], rewards[(i, 'L')]))\n",
        "    else:  # Switch to a different stock\n",
        "        if stock_states[1 - (i - 1)] == 'H':\n",
        "            next_states.append(((2 if i == 1 else 1, stock_states[1 - (i - 1)], stock_states[i - 1]), transitions[(2 if i == 1 else 1, 'H')]['H'], rewards[(2 if i == 1 else 1, 'H')] - c))\n",
        "            stock_states[1 - (i - 1)] = 'L'\n",
        "            next_states.append(((2 if i == 1 else 1, stock_states[1 - (i - 1)], stock_states[i - 1]), transitions[(2 if i == 1 else 1, 'H')]['L'], rewards[(2 if i == 1 else 1, 'H')] - c))\n",
        "        else:\n",
        "            next_states.append(((2 if i == 1 else 1, stock_states[1 - (i - 1)], stock_states[i - 1]), transitions[(2 if i == 1 else 1, 'L')]['H'], rewards[(2 if i == 1 else 1, 'L')] - c))\n",
        "            stock_states[1 - (i - 1)] = 'H'\n",
        "            next_states.append(((2 if i == 1 else 1, stock_states[1 - (i - 1)], stock_states[i - 1]), transitions[(2 if i == 1 else 1, 'L')]['L'], rewards[(2 if i == 1 else 1, 'L')] - c))\n",
        "\n",
        "    return next_states\n",
        "\n",
        "def policy_evaluation(policy, V, theta=1e-6):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in states:\n",
        "            v = V[get_state_index(state)]\n",
        "            action = policy[get_state_index(state)]\n",
        "            new_v = 0\n",
        "            for next_state, prob, reward in get_transition_prob_reward(state, action):\n",
        "                new_v += prob * (reward + gamma * V[get_state_index(next_state)])\n",
        "            V[get_state_index(state)] = new_v\n",
        "            delta = max(delta, abs(v - new_v))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def policy_improvement(V, policy):\n",
        "    policy_stable = True\n",
        "    new_policy = policy.copy()\n",
        "    for state in states:\n",
        "        old_action = policy[get_state_index(state)]\n",
        "        action_values = np.zeros(len(actions))\n",
        "        for a_idx, action in enumerate(actions):\n",
        "            for next_state, prob, reward in get_transition_prob_reward(state, action):\n",
        "                action_values[a_idx] += prob * (reward + gamma * V[get_state_index(next_state)])\n",
        "        new_action = actions[np.argmax(action_values)]\n",
        "        new_policy[get_state_index(state)] = new_action\n",
        "        if new_action != old_action:\n",
        "            policy_stable = False\n",
        "    return new_policy, policy_stable\n",
        "\n",
        "def policy_iteration():\n",
        "    V = np.zeros(len(states))\n",
        "    policy = np.random.choice(actions, len(states))\n",
        "    V_History = []\n",
        "\n",
        "    while True:\n",
        "        V = policy_evaluation(policy, V)\n",
        "        policy, policy_stable = policy_improvement(V, policy)\n",
        "        V_History.append(np.copy(V))\n",
        "        if policy_stable:\n",
        "            break\n",
        "    return policy, V, V_History\n",
        "\n",
        "optimal_policy, V, V_History = policy_iteration()\n",
        "\n",
        "# Display the optimal policy\n",
        "print(\"Optimal Policy:\")\n",
        "for state in states:\n",
        "    print(f\"State {state}: {optimal_policy[get_state_index(state)]}\")\n",
        "\n",
        "print(\"State Values:\")\n",
        "for state in states:\n",
        "    print(f\"State {state}: {V[get_state_index(state)]:.4f}\")\n"
      ],
      "metadata": {
        "id": "UKXw99yhxg6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q-Learning Algorithm for this scenario"
      ],
      "metadata": {
        "id": "bHTlzdh4x6UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "# Parameters\n",
        "N = 2  # Number of stocks\n",
        "gamma = 0  # Discount factor\n",
        "alpha = 0.1  # Learning rate\n",
        "epsilon = 0.1  # Exploration rate\n",
        "num_episodes = 10000  # Number of episodes for training\n",
        "transaction_cost = 0.12  # Flat transaction fee\n",
        "\n",
        "# Reward parameters\n",
        "rewards = {\n",
        "    1: {'H': 0.1, 'L': -0.02},\n",
        "    2: {'H': 0.05, 'L': -0.01}\n",
        "}\n",
        "\n",
        "# Transition probabilities\n",
        "transition_probs = {\n",
        "    1: {'H': {'H': 0.9, 'L': 0.1}, 'L': {'L': 0.9, 'H': 0.1}},\n",
        "    2: {'H': {'H': 0.9, 'L': 0.1}, 'L': {'L': 0.9, 'H': 0.1}}\n",
        "}\n",
        "\n",
        "# Generate all possible states\n",
        "def generate_states(N):\n",
        "    states = []\n",
        "    for i in range(1, N + 1):\n",
        "        for stock_states in itertools.product(['H', 'L'], repeat=N):\n",
        "            states.append((i, *stock_states))\n",
        "    return states\n",
        "\n",
        "states = generate_states(N)\n",
        "actions = list(range(1, N + 1))\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = {state: {action: 0 for action in actions} for state in states}\n",
        "\n",
        "# Define the reward function based on the provided parameters\n",
        "def get_reward(state, action):\n",
        "    current_stock, *stock_states = state\n",
        "    current_state = stock_states[current_stock - 1]\n",
        "    reward = rewards[action][current_state]\n",
        "    if action != current_stock:\n",
        "        reward -= transaction_cost  # Apply transaction cost\n",
        "    return reward\n",
        "\n",
        "# Define the transition function based on the provided parameters\n",
        "def get_next_state(state, action):\n",
        "    current_stock, *stock_states = state\n",
        "    next_stock_states = []\n",
        "    for i in range(N):\n",
        "        current_state = stock_states[i]\n",
        "        next_state = 'H' if random.random() < transition_probs[i + 1][current_state]['H'] else 'L'\n",
        "        next_stock_states.append(next_state)\n",
        "    next_state = (action, *next_stock_states)\n",
        "    return next_state\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # Initialize the state\n",
        "    state = random.choice(states)\n",
        "\n",
        "    for t in range(100):  # Assume a finite horizon for each episode\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice(actions)  # Explore\n",
        "        else:\n",
        "            action = max(Q[state], key=Q[state].get)  # Exploit\n",
        "\n",
        "        # Get the reward and next state\n",
        "        reward = get_reward(state, action)\n",
        "        next_state = get_next_state(state, action)\n",
        "\n",
        "        # Update Q-value\n",
        "        best_next_action = max(Q[next_state], key=Q[next_state].get)\n",
        "        Q[state][action] += alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][action])\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "# Extract the optimal policy and state values\n",
        "optimal_policy = {}\n",
        "state_values = {}\n",
        "for state in states:\n",
        "    best_action = max(Q[state], key=Q[state].get)\n",
        "    if state[0] == best_action:\n",
        "        optimal_policy[state] = 'keep'\n",
        "    else:\n",
        "        optimal_policy[state] = f'switch to stock {best_action}'\n",
        "    state_values[state] = max(Q[state].values())\n",
        "\n",
        "# Print the optimal policy and state values\n",
        "print(\"Optimal Policy:\")\n",
        "for state, action in optimal_policy.items():\n",
        "    print(f\"State {state}: {action}\")\n",
        "\n",
        "print(\"State Values:\")\n",
        "for state, value in state_values.items():\n",
        "    print(f\"State {state}: {value:.4f}\")\n"
      ],
      "metadata": {
        "id": "C-ExXc_fyD0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scenario of Assignment 3 Question 2\n",
        "###Code from Assignment 3 (Policy Iteration - For Comparison)"
      ],
      "metadata": {
        "id": "RaaU49fcyLdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "gamma = 0.9\n",
        "r1H = 0.1\n",
        "r1L = -0.02\n",
        "r2H = 0.05\n",
        "r2L = -0.01\n",
        "c = 0.03\n",
        "p1HH = 0.9\n",
        "p1HL = 0.1\n",
        "p1LL = 0.9\n",
        "p1LH = 0.1\n",
        "p2HH = 0.9\n",
        "p2HL = 0.1\n",
        "p2LL = 0.9\n",
        "p2LH = 0.1\n",
        "N = 2\n",
        "\n",
        "rewards = {\n",
        "    (1, 'H'): r1H,\n",
        "    (1, 'L'): r1L,\n",
        "    (2, 'H'): r2H,\n",
        "    (2, 'L'): r2L\n",
        "}\n",
        "\n",
        "transitions = {\n",
        "    (1, 'H'): {'H': p1HH, 'L': p1HL},\n",
        "    (1, 'L'): {'H': p1LH, 'L': p1LL},\n",
        "    (2, 'H'): {'H': p2HH, 'L': p2HL},\n",
        "    (2, 'L'): {'H': p2LH, 'L': p2LL}\n",
        "}\n",
        "\n",
        "states = [(1, 'H', 'H'), (1, 'H', 'L'), (1, 'L', 'H'), (1, 'L', 'L'),\n",
        "          (2, 'H', 'H'), (2, 'H', 'L'), (2, 'L', 'H'), (2, 'L', 'L')]\n",
        "\n",
        "actions = ['keep', 'switch']\n",
        "\n",
        "def get_state_index(state):\n",
        "    return states.index(state)\n",
        "\n",
        "def get_transition_prob_reward(state, action):\n",
        "    i, stock_state1, stock_state2 = state\n",
        "    next_states = []\n",
        "    stock_states = [stock_state1, stock_state2]\n",
        "\n",
        "    if action == 'keep':\n",
        "        if stock_states[i - 1] == 'H':\n",
        "            next_states.append(((i, stock_states[i - 1], stock_states[1 - (i - 1)]), transitions[(i, 'H')]['H'], rewards[(i, 'H')]))\n",
        "            stock_states[i - 1] = 'L'\n",
        "            next_states.append(((i, stock_states[i - 1], stock_states[1 - (i - 1)]), transitions[(i, 'H')]['L'], rewards[(i, 'H')]))\n",
        "        else:\n",
        "            next_states.append(((i, stock_states[i - 1], stock_states[1 - (i - 1)]), transitions[(i, 'L')]['H'], rewards[(i, 'L')]))\n",
        "            stock_states[i - 1] = 'H'\n",
        "            next_states.append(((i, stock_states[i - 1], stock_states[1 - (i - 1)]), transitions[(i, 'L')]['L'], rewards[(i, 'L')]))\n",
        "    else:  # Switch to a different stock\n",
        "        if stock_states[1 - (i - 1)] == 'H':\n",
        "            next_states.append(((2 if i == 1 else 1, stock_states[1 - (i - 1)], stock_states[i - 1]), transitions[(2 if i == 1 else 1, 'H')]['H'], rewards[(2 if i == 1 else 1, 'H')] - c))\n",
        "            stock_states[1 - (i - 1)] = 'L'\n",
        "            next_states.append(((2 if i == 1 else 1, stock_states[1 - (i - 1)], stock_states[i - 1]), transitions[(2 if i == 1 else 1, 'H')]['L'], rewards[(2 if i == 1 else 1, 'H')] - c))\n",
        "        else:\n",
        "            next_states.append(((2 if i == 1 else 1, stock_states[1 - (i - 1)], stock_states[i - 1]), transitions[(2 if i == 1 else 1, 'L')]['H'], rewards[(2 if i == 1 else 1, 'L')] - c))\n",
        "            stock_states[1 - (i - 1)] = 'H'\n",
        "            next_states.append(((2 if i == 1 else 1, stock_states[1 - (i - 1)], stock_states[i - 1]), transitions[(2 if i == 1 else 1, 'L')]['L'], rewards[(2 if i == 1 else 1, 'L')] - c))\n",
        "\n",
        "    return next_states\n",
        "\n",
        "def policy_evaluation(policy, V, theta=1e-6):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in states:\n",
        "            v = V[get_state_index(state)]\n",
        "            action = policy[get_state_index(state)]\n",
        "            new_v = 0\n",
        "            for next_state, prob, reward in get_transition_prob_reward(state, action):\n",
        "                new_v += prob * (reward + gamma * V[get_state_index(next_state)])\n",
        "            V[get_state_index(state)] = new_v\n",
        "            delta = max(delta, abs(v - new_v))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def policy_improvement(V, policy):\n",
        "    policy_stable = True\n",
        "    new_policy = policy.copy()\n",
        "    for state in states:\n",
        "        old_action = policy[get_state_index(state)]\n",
        "        action_values = np.zeros(len(actions))\n",
        "        for a_idx, action in enumerate(actions):\n",
        "            for next_state, prob, reward in get_transition_prob_reward(state, action):\n",
        "                action_values[a_idx] += prob * (reward + gamma * V[get_state_index(next_state)])\n",
        "        new_action = actions[np.argmax(action_values)]\n",
        "        new_policy[get_state_index(state)] = new_action\n",
        "        if new_action != old_action:\n",
        "            policy_stable = False\n",
        "    return new_policy, policy_stable\n",
        "\n",
        "def policy_iteration():\n",
        "    V = np.zeros(len(states))\n",
        "    policy = np.random.choice(actions, len(states))\n",
        "    V_History = []\n",
        "\n",
        "    while True:\n",
        "        V = policy_evaluation(policy, V)\n",
        "        policy, policy_stable = policy_improvement(V, policy)\n",
        "        V_History.append(np.copy(V))\n",
        "        if policy_stable:\n",
        "            break\n",
        "    return policy, V, V_History\n",
        "\n",
        "optimal_policy, V, V_History = policy_iteration()\n",
        "\n",
        "# Display the optimal policy\n",
        "print(\"Optimal Policy:\")\n",
        "for state in states:\n",
        "    print(f\"State {state}: {optimal_policy[get_state_index(state)]}\")\n",
        "\n",
        "print(\"State Values:\")\n",
        "for state in states:\n",
        "    print(f\"State {state}: {V[get_state_index(state)]:.4f}\")\n"
      ],
      "metadata": {
        "id": "mlsIeTXIyOw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q-Learning Algorithm for this scenario"
      ],
      "metadata": {
        "id": "2Mu_hfJ2ySrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "# Parameters\n",
        "N = 2  # Number of stocks\n",
        "gamma = 0.9  # Discount factor\n",
        "alpha = 0.1  # Learning rate\n",
        "epsilon = 1  # Exploration rate\n",
        "epsilon_decay = 0.995  # Decay rate for exploration rate\n",
        "num_episodes = 1000 # Number of episodes for training\n",
        "transaction_cost = 0.03  # Flat transaction fee\n",
        "\n",
        "# Reward parameters\n",
        "rewards = {\n",
        "    1: {'H': 0.1, 'L': -0.02},\n",
        "    2: {'H': 0.05, 'L': -0.01}\n",
        "}\n",
        "\n",
        "# Transition probabilities\n",
        "transition_probs = {\n",
        "    1: {'H': {'H': 0.9, 'L': 0.1}, 'L': {'L': 0.9, 'H': 0.1}},\n",
        "    2: {'H': {'H': 0.9, 'L': 0.1}, 'L': {'L': 0.9, 'H': 0.1}}\n",
        "}\n",
        "\n",
        "# Generate all possible states\n",
        "def generate_states(N):\n",
        "    states = []\n",
        "    for i in range(1, N + 1):\n",
        "        for stock_states in itertools.product(['H', 'L'], repeat=N):\n",
        "            states.append((i, *stock_states))\n",
        "    return states\n",
        "\n",
        "states = generate_states(N)\n",
        "actions = list(range(1, N + 1))\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = {state: {action: 0 for action in actions} for state in states}\n",
        "\n",
        "# Define the reward function based on the provided parameters\n",
        "def get_reward(state, action):\n",
        "    current_stock, *stock_states = state\n",
        "    current_state = stock_states[current_stock - 1]\n",
        "    reward = rewards[action][current_state]\n",
        "    if action != current_stock:\n",
        "        reward -= transaction_cost  # Apply transaction cost\n",
        "    return reward\n",
        "\n",
        "# Define the transition function based on the provided parameters\n",
        "def get_next_state(state, action):\n",
        "    current_stock, *stock_states = state\n",
        "    next_stock_states = []\n",
        "    for i in range(N):\n",
        "        current_state = stock_states[i]\n",
        "        next_state = 'H' if random.random() < transition_probs[i + 1][current_state]['H'] else 'L'\n",
        "        next_stock_states.append(next_state)\n",
        "    next_state = (action, *next_stock_states)\n",
        "    return next_state\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # Initialize the state\n",
        "    state = random.choice(states)\n",
        "\n",
        "    for t in range(500):  # Assume a finite horizon for each episode\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice(actions)  # Explore\n",
        "        else:\n",
        "            action = max(Q[state], key=Q[state].get)  # Exploit\n",
        "\n",
        "        # Get the reward and next state\n",
        "        reward = get_reward(state, action)\n",
        "        next_state = get_next_state(state, action)\n",
        "\n",
        "        # Update Q-value\n",
        "        best_next_action = max(Q[next_state], key=Q[next_state].get)\n",
        "        Q[state][action] += alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][action])\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "    epsilon *= epsilon_decay\n",
        "\n",
        "# Extract the optimal policy and state values\n",
        "optimal_policy = {}\n",
        "state_values = {}\n",
        "for state in states:\n",
        "    best_action = max(Q[state], key=Q[state].get)\n",
        "    if state[0] == best_action:\n",
        "        optimal_policy[state] = 'keep'\n",
        "    else:\n",
        "        optimal_policy[state] = f'switch to stock {best_action}'\n",
        "    state_values[state] = max(Q[state].values())\n",
        "\n",
        "# Print the optimal policy and state values\n",
        "print(\"Optimal Policy:\")\n",
        "for state, action in optimal_policy.items():\n",
        "    print(f\"State {state}: {action}\")\n",
        "\n",
        "print(\"State Values:\")\n",
        "for state, value in state_values.items():\n",
        "    print(f\"State {state}: {value:.4f}\")\n"
      ],
      "metadata": {
        "id": "4ITORSNryXaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2 (8 Stocks)\n",
        "##Scenario of Assignment 3 Question 2\n",
        "###Code from Assignment 3 (Policy Iteration - For Comparison)\n",
        "**IMPORTANT: EACH TIME IT RUNS IT GENERATES THE TRANSITION PROBABILITIES AND REWARDS OF THE STOCKS CSV FILE, THAT IS USED FOR ENVIRONMENT INITIALIZATION IN TASK 2 (Q-LEARNING) AND TASK 3 (DQN). CHECK THE REPORT FOR MORE INFO**"
      ],
      "metadata": {
        "id": "FADXj2-iylMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import itertools\n",
        "\n",
        "# Environment parameters\n",
        "N = 8  # Number of stocks, you can change this to any value\n",
        "c = 0.1  # Transaction fee\n",
        "gamma = 0.99  # Discount factor\n",
        "\n",
        "# Reward and transition probabilities\n",
        "#np.random.seed(0)  # For reproducibility\n",
        "r_H = np.random.uniform(-0.02, 0.1, N)\n",
        "r_L = np.random.uniform(-0.02, 0.1, N)\n",
        "p_HL = np.array([0.1 if i < N // 2 else 0.5 for i in range(N)])\n",
        "p_LH = np.array([0.1 if i < N // 2 else 0.5 for i in range(N)])\n",
        "p_HH = 1 - p_HL\n",
        "p_LL = 1 - p_LH\n",
        "\n",
        "# State space\n",
        "states = list(itertools.product(range(N), itertools.product(['H', 'L'], repeat=N)))\n",
        "\n",
        "# Action space\n",
        "actions = list(range(N))\n",
        "\n",
        "# Helper function to get the state index\n",
        "def get_state_index(state):\n",
        "    return states.index(state)\n",
        "\n",
        "# Transition probabilities and rewards\n",
        "def get_transition_prob_reward(state, action):\n",
        "    i, stock_states = state\n",
        "    next_states = []\n",
        "    stock_states = list(stock_states)\n",
        "    if action == i:  # Stay with the current stock\n",
        "        if stock_states[action] == 'H':\n",
        "            next_states.append(((i, tuple(stock_states)), p_HH[action], r_H[action]))\n",
        "            stock_states[action] = 'L'\n",
        "            next_states.append(((i, tuple(stock_states)), p_HL[action], r_H[action]))\n",
        "        else:\n",
        "            next_states.append(((i, tuple(stock_states)), p_LH[action], r_L[action]))\n",
        "            stock_states[action] = 'H'\n",
        "            next_states.append(((i, tuple(stock_states)), p_LL[action], r_L[action]))\n",
        "    else:  # Switch to a different stock\n",
        "        if stock_states[action] == 'H':\n",
        "            next_states.append(((action, tuple(stock_states)), p_HH[action], r_H[action] - c))\n",
        "            stock_states[action] = 'L'\n",
        "            next_states.append(((action, tuple(stock_states)), p_HL[action], r_H[action] - c))\n",
        "        else:\n",
        "            next_states.append(((action, tuple(stock_states)), p_LH[action], r_L[action] - c))\n",
        "            stock_states[action] = 'H'\n",
        "            next_states.append(((action, tuple(stock_states)), p_LL[action], r_L[action] - c))\n",
        "    return next_states\n",
        "\n",
        "# Policy Iteration\n",
        "def policy_evaluation(policy, V, theta=1e-6):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in states:\n",
        "            v = V[get_state_index(state)]\n",
        "            action = policy[get_state_index(state)]\n",
        "            new_v = 0\n",
        "            for next_state, prob, reward in get_transition_prob_reward(state, action):\n",
        "                new_v += prob * (reward + gamma * V[get_state_index(next_state)])\n",
        "            V[get_state_index(state)] = new_v\n",
        "            delta = max(delta, abs(v - new_v))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def policy_improvement(V, policy):\n",
        "    policy_stable = True\n",
        "    new_policy = policy.copy()\n",
        "    for state in states:\n",
        "        old_action = policy[get_state_index(state)]\n",
        "        action_values = np.zeros(len(actions))\n",
        "        for action in actions:\n",
        "            for next_state, prob, reward in get_transition_prob_reward(state, action):\n",
        "                action_values[action] += prob * (reward + gamma * V[get_state_index(next_state)])\n",
        "        new_action = np.argmax(action_values)\n",
        "        new_policy[get_state_index(state)] = new_action\n",
        "        if new_action != old_action:\n",
        "            policy_stable = False\n",
        "    return new_policy, policy_stable\n",
        "\n",
        "def policy_iteration():\n",
        "    V = np.zeros(len(states))\n",
        "    policy = np.random.choice(actions, len(states))\n",
        "\n",
        "    while True:\n",
        "        V = policy_evaluation(policy, V)\n",
        "        policy, policy_stable = policy_improvement(V, policy)\n",
        "        if policy_stable:\n",
        "            break\n",
        "    return policy, V\n",
        "\n",
        "optimal_policy, V = policy_iteration()\n",
        "\n",
        "# Display the optimal policy\n",
        "print(\"Optimal Policy:\")\n",
        "for state in states:\n",
        "    print(f\"State {state}: Invest in stock {optimal_policy[get_state_index(state)]}\")\n",
        "\n",
        "print(\"State Values:\")\n",
        "for state in states:\n",
        "    print(f\"State {state}: {V[get_state_index(state)]:.4f}\")\n",
        "\n",
        "# Output optimal policy and state values to CSV\n",
        "output_file_policy = 'optimal_policy.csv'\n",
        "with open(output_file_policy, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"State\", \"Optimal Action\", \"State Value\"])\n",
        "    for state in states:\n",
        "        writer.writerow([state, optimal_policy[get_state_index(state)], V[get_state_index(state)]])\n",
        "\n",
        "# Output transition probabilities and rewards to CSV\n",
        "output_file_params = 'stock_params.csv'\n",
        "with open(output_file_params, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Stock\", \"p_HH\", \"p_LH\", \"p_LL\", \"p_HL\", \"r_H\", \"r_L\"])\n",
        "    for i in range(N):\n",
        "        writer.writerow([i, p_HH[i], p_LH[i], p_LL[i], p_HL[i], r_H[i], r_L[i]])\n",
        "\n",
        "print(f\"Results have been written to {output_file_policy} and {output_file_params}\")\n"
      ],
      "metadata": {
        "id": "FD534BDMywTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q-Learning Algorithm for this scenario"
      ],
      "metadata": {
        "id": "B7u8nTxLzc7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters\n",
        "N = 8  # Adjust based on your actual problem\n",
        "gamma = 0.99  # Discount factor\n",
        "alpha = 0.1  # Learning rate\n",
        "epsilon = 0.1  # Exploration rate\n",
        "num_episodes = 15000  # Number of episodes for training\n",
        "\n",
        "# Initialize transition probabilities and rewards from CSV file\n",
        "def load_params_from_csv(file_path):\n",
        "    p_HH, p_LH, p_LL, p_HL, r_H, r_L = [], [], [], [], [], []\n",
        "    with open(file_path, mode='r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            p_HH.append(float(row['p_HH']))\n",
        "            p_LH.append(float(row['p_LH']))\n",
        "            p_LL.append(float(row['p_LL']))\n",
        "            p_HL.append(float(row['p_HL']))\n",
        "            r_H.append(float(row['r_H']))\n",
        "            r_L.append(float(row['r_L']))\n",
        "    return np.array(p_HH), np.array(p_LH), np.array(p_LL), np.array(p_HL), np.array(r_H), np.array(r_L)\n",
        "\n",
        "p_HH, p_LH, p_LL, p_HL, r_H, r_L = load_params_from_csv('stock_params.csv')\n",
        "\n",
        "# Generate all possible states\n",
        "def generate_states(N):\n",
        "    states = []\n",
        "    for i in range(1, N + 1):\n",
        "        for stock_states in itertools.product(['H', 'L'], repeat=N):\n",
        "            states.append((i, *stock_states))\n",
        "    return states\n",
        "\n",
        "states = generate_states(N)\n",
        "actions = list(range(1, N + 1))\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = {state: {action: 0 for action in actions} for state in states}\n",
        "\n",
        "# Define the reward function using loaded parameters\n",
        "def get_reward(state, action, c=0.1):\n",
        "    i, *stock_states = state\n",
        "    reward = r_H[action - 1] if stock_states[action - 1] == 'H' else r_L[action - 1]\n",
        "    if action != i:\n",
        "        reward -= c  # Apply transaction cost\n",
        "    return reward\n",
        "\n",
        "# Define the transition function using loaded parameters\n",
        "def get_next_state(state, action):\n",
        "    i, *stock_states = state\n",
        "    next_stock_states = []\n",
        "    for j, stock_state in enumerate(stock_states):\n",
        "        if stock_state == 'H':\n",
        "            next_stock_states.append('H' if random.random() < p_HH[j] else 'L')\n",
        "        else:\n",
        "            next_stock_states.append('H' if random.random() < p_LH[j] else 'L')\n",
        "    next_state = (action, *next_stock_states)\n",
        "    return next_state\n",
        "\n",
        "# Lists to track metrics\n",
        "average_rewards_per_episode = []\n",
        "average_value_functions_per_episode = []\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # Initialize the state\n",
        "    state = random.choice(states)\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(10000):  # Assume a finite horizon for each episode\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice(actions)  # Explore\n",
        "        else:\n",
        "            action = max(Q[state], key=Q[state].get)  # Exploit\n",
        "\n",
        "        # Get the reward and next state\n",
        "        reward = get_reward(state, action)\n",
        "        next_state = get_next_state(state, action)\n",
        "\n",
        "        # Update Q-value\n",
        "        best_next_action = max(Q[next_state], key=Q[next_state].get)\n",
        "        Q[state][action] += alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][action])\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Accumulate reward\n",
        "        total_reward += reward\n",
        "\n",
        "    # Compute the average reward for this episode\n",
        "    average_rewards_per_episode.append(total_reward / 100)\n",
        "\n",
        "    # Compute the average value function for this episode\n",
        "    average_value_function = np.mean([max(Q[s].values()) for s in states])\n",
        "    average_value_functions_per_episode.append(average_value_function)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot the average reward per episode\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(average_rewards_per_episode, label='Average Reward per Episode')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title('Average Reward per Episode')\n",
        "\n",
        "# Plot the average value function per episode\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(average_value_functions_per_episode, label='Average Value Function per Episode')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Average Value Function')\n",
        "plt.title('Average Value Function per Episode')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Extract the optimal policy and state values\n",
        "optimal_policy = {}\n",
        "state_values = {}\n",
        "for state in states:\n",
        "    best_action = max(Q[state], key=Q[state].get)\n",
        "    if state[0] == best_action:\n",
        "        optimal_policy[state] = 'keep'\n",
        "    else:\n",
        "        optimal_policy[state] = f'switch to stock {best_action}'\n",
        "    state_values[state] = max(Q[state].values())\n",
        "\n",
        "# Print the optimal policy and state values\n",
        "print(\"Optimal Policy:\")\n",
        "for state, action in optimal_policy.items():\n",
        "    print(f\"State {state}: {action}\")\n",
        "\n",
        "print(\"State Values:\")\n",
        "for state, value in state_values.items():\n",
        "    print(f\"State {state}: {value:.4f}\")\n"
      ],
      "metadata": {
        "id": "pBAD-M0qzbrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 3\n",
        "##DQN"
      ],
      "metadata": {
        "id": "WdVAcx_c0hvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the stock parameters from the CSV file\n",
        "file_path = 'stock_params.csv'\n",
        "stock_params = pd.read_csv(file_path)\n",
        "\n",
        "# Display the loaded stock parameters\n",
        "print(stock_params)\n",
        "\n",
        "class StockMarketEnv:\n",
        "    def __init__(self, stock_params, transaction_fee, gamma):\n",
        "        self.stock_params = stock_params\n",
        "        self.transaction_fee = transaction_fee\n",
        "        self.gamma = gamma\n",
        "        self.num_stocks = stock_params.shape[0]\n",
        "        self.state = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Initialize the state with random yields\n",
        "        self.state = np.random.rand(self.num_stocks)\n",
        "        return torch.tensor(self.state, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    def step(self, action):\n",
        "        current_stock = action\n",
        "        next_state = []\n",
        "        reward = 0\n",
        "\n",
        "        for i in range(self.num_stocks):\n",
        "            if random.random() < self.stock_params.iloc[i]['p_HH'] if self.state[i] > 0.5 else self.stock_params.iloc[i]['p_LL']:\n",
        "                next_state.append(self.stock_params.iloc[i]['r_H'] if self.state[i] > 0.5 else self.stock_params.iloc[i]['r_L'])\n",
        "            else:\n",
        "                next_state.append(self.stock_params.iloc[i]['r_L'] if self.state[i] > 0.5 else self.stock_params.iloc[i]['r_H'])\n",
        "\n",
        "        reward = next_state[current_stock]\n",
        "\n",
        "        if action != current_stock:\n",
        "            reward -= self.transaction_fee\n",
        "\n",
        "        self.state = next_state\n",
        "        done = False  # In this environment, the episode does not terminate\n",
        "        return torch.tensor(next_state, dtype=torch.float32).unsqueeze(0), reward, done, {}\n",
        "\n",
        "# Initialize the environment\n",
        "env = StockMarketEnv(stock_params, transaction_fee=0.1, gamma=0.99)\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the DQN\n",
        "input_dim = env.num_stocks\n",
        "output_dim = env.num_stocks\n",
        "dqn = DQN(input_dim, output_dim)\n",
        "\n",
        "def train_dqn(env, dqn, num_episodes, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay, lr):\n",
        "    optimizer = optim.Adam(dqn.parameters(), lr=lr)\n",
        "    memory = []\n",
        "    epsilon = epsilon_start\n",
        "    all_rewards = []\n",
        "    all_value_functions = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_rewards = 0\n",
        "\n",
        "        for t in range(100):  # Limit the number of steps per episode\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice(range(env.num_stocks))\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action = dqn(state).argmax().item()\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_rewards += reward\n",
        "\n",
        "            memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "            if len(memory) > batch_size:\n",
        "                minibatch = random.sample(memory, batch_size)\n",
        "                states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "                states = torch.cat(states)\n",
        "                next_states = torch.cat(next_states)\n",
        "\n",
        "                q_values = dqn(states)\n",
        "                next_q_values = dqn(next_states)\n",
        "\n",
        "                q_target = torch.tensor(rewards, dtype=torch.float32) + gamma * next_q_values.max(1)[0] * (1 - torch.tensor(dones, dtype=torch.float32))\n",
        "\n",
        "                q_values = q_values.gather(1, torch.tensor(actions, dtype=torch.int64).unsqueeze(1)).squeeze()\n",
        "\n",
        "                loss = F.mse_loss(q_values, q_target)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
        "        all_rewards.append(episode_rewards)\n",
        "        all_value_functions.append(dqn(state).max().item())\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            print(f'Episode {episode}, Reward: {episode_rewards}, Epsilon: {epsilon}')\n",
        "\n",
        "    return all_rewards, all_value_functions\n",
        "\n",
        "# Train the DQN agent\n",
        "num_episodes = 1000\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.1\n",
        "epsilon_decay = 0.995\n",
        "lr = 1e-3\n",
        "\n",
        "rewards, value_functions = train_dqn(env, dqn, num_episodes, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay, lr)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot the average reward per episode\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title('Average Reward per Episode')\n",
        "\n",
        "# Plot the average value function per episode\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(value_functions)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Average Value Function')\n",
        "plt.title('Average Value Function per Episode')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IkqOpmGU0oCY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}