Environment (Investment Model): 

At every round you will invest exactly 1 euro. And at the end of the day you will receive as profit the percentage increase of the respective stock (i.e., if your stock had a 5% increase, you'll receive 5 cents that day. Your goal is once more to maximize your cumulative profit in the long run.
If you choose to invest your 1 euro on the same stock, then no transaction fees must be paid. But, if you choose to switch stock, then a flat transaction fee of c euros must be paid also (c ∈ [0,1]).
Unless otherwise stated, you keep playing this game for an infinite horizon T, and inflation is captured in the discount factor γ (the higher it is the smaller the inflation).

Environment (Stock price Evolution:) There are N stocks available, and the price of each stock i evolves as a 2-state Markov Chain, between a  "H(igh)" and a "L(ow)" price as follows (attached are the markov chains): 

When at state H, the expected gain of the stock is r_{i}^{H}. With probability p_{i}^{HH} the stock will remain in state H, and with probability p_{i}^{HL} the stock will go to state L.
When at state L, the expected gain of the stock is r_{i}^{L}. With probability p_{i}^{LL} the stock will remain in state L, and with probability p_{i}^{LH} the stock will go to state H.

Agent (Control Variables and Objective):

At time instance t, your algorithm must choose which stock to place its 1 euro. 
Objective: Maximize the expected (discounted) long-term profits, starting from any initial state.



Implement now a Deep Q Learning method to solve the problem. You can implement DQN, Actor-Critic (or other Deep RL variants).

IMPORTANT Note: Traffic should still be Markov chain based as before, again with unknown parameters. Your agent does not know that there are two states (hence you cannot use H,L as part of the ``state'' of your algorithm...you just see continuous values corresponding to the yield of each stock, having no idea what underlying process is guiding these). 
